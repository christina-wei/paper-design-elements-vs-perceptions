%%%%%
\subsection{Perception of Interaction with an Agent}
%%%%%

This section discusses user's perception of their interaction with a conversational agent. \textbf{\textit{Engagement, accuracy, and satisfaction are the three variables used to evaluate how people perceive interactions with CAs}}. \textit{Engagement} measures user's attitude towards the task they are achieving through working with the agent, including how easy it is to use the system and its perceived usefulness, convenience and helpfulness for the user. Additionally, it includes the user's emotional responses to the interaction, such as annoyance, enjoyment, and irritation. \textit{Accuracy} measures whether the agent was perceived to be able to provide the correct answers in response to the user. \textit{Satisfaction} measures the whether the agent was able to fulfill user's expectations through the interaction encounter.

Out of the list of publications that were evaluated, there is a good coverage on the effect of conversational architecture elements on the perception of interaction with an agent. Out of the three factors, engagement is the most frequently used measure to assess interaction quality, followed by satisfaction. Perceived accuracy of the agent is rarely used within literature, with only a few papers measuring this perception. 

[TODO] * Associations between conversational architecture elements and perceptions
For both text and voice modalities, lexical alignment had a positive effect on engagement by decreasing the cognitive load of users. 

%%
\subsubsection{Engagement}
%%
For the elements in the language content category of conversational architecture, the use \textbf{self-disclosure} by agent had a positive effect on engagement, as the participants using the chatbot with small talk and self disclosure reported higher enjoyment after 3 weeks of daily interaction \cite{lee2020hear}\cmt{[23]}. There are mixed results on the effect of \textbf{affective language} on engagement. One study found that CAs that uses encouraging words are rated as easier to use \cite{healey2013relating}\cmt{[39]}. However, the user of words with increased lexical emotional expressiveness did not have an effect on engagement \cite{zhu2022effects}\cmt{[26]}. The use of affective language may lead to negative impacts on engagement, as a system that uses an extroverted and overly familiar words is irritating to users. On the use of \textbf{humour}, survey result did not show a significant difference in the level of enjoyment, but qualitative feedback from participants reflected that humour made the interaction more engaging and immersive \cite{ceha2021can}\cmt{[57]}.

For the elements in the language style category of conversational architecture, mimicking users' utterances through \textbf{lexical alignment} yielded positive results. Multiple studies have found that being lexically aligned to users resulted in lower perceived cognitive demand and decreased perceived workload \cite{huiyang2022improving}\cmt{[17]}\cite{linnemann2018can}\cmt{[15]}\cite{spillner2021talk}\cmt{[18]}. Interestingly, perceived lower cognitive demand did not have significant impacts on ease of use \cite{linnemann2018can}\cmt{[15]} or annoyance \cite{huiyang2022improving}\cmt{[17]}. In Spillner et al's study \cite{spillner2021talk}\cmt{[18]}, both conditions of aligning words as well as aligning words and grammatical structure to user utterances resulted in higher perceived user engagement from participants. 

For the elements in the presentation format category of conversational architecture, auditory aspects of \textbf{prosody} such as speech rate and expressive tone of voice had different impacts on engagement. In one study, users enjoyed their interaction with the CA and perceived it as easier to use, as they felt the level of agent's social behaviour is matched with their emotional states \cite{kim2020can}\cmt{[24]}. However, another study did not find any impact of emotional expressive of voice on the user engagement \cite{zhu2022effects}\cmt{[26]}. On the speech rate for CAs, a normal rate is perceived as more convenient compared to a faster rate of speech \cite{choi2020nobody}\cmt{[54]}. On the visual presentation side, the study from Westerman et al. \cite{westerman2019believe}\cmt{[9]} showed that the use of \textbf{typos} by chatbots negatively impacted the perception of working with a CA on the specific task, while the use of \textbf{capitalization} did not yield any significant impacts.

\textbf{Content: Humour}
* No overall significant difference between conventional method vs. agent using humour for enjoyment. Specifically, men enjoyed the agent's jokes more than women.  \cite{miyamoto2017improving}\cmt{[46]}.
* Motivation to continue the conversation significant increased after day 2 for the humorous agent \cite{go2021conversational}\cmt{[80]}

\textbf{Style: Language Formality}
* Users spent more time with the Shakespearean chatbot version with more non-task related user inputs. Enjoyment was measured but no report \cite{elsholz2019exploring}\cmt{[61]}
* commanding/formal wording was rated significantly more useful than the informal wording; while the informal wording was rated as significantly more annoying than the commanding/formal wording. No impact on desirability. \cite{jestin2022effects}\cmt{[81]}
* High conscientiousness (formal) and high extroversion (casual) chatbot personality have higher user engagement. \cite{moilanen2022measuring}\cmt{[82]}
* Analysis of ease of use reveals that there are no significant main effects for platform questionnaire vs. chatbot, and conversational style formal vs. casual.\cite{kim2019comparing}\cmt{[89]}

\textbf{Social talk / elaborate}
* Not significant importance between introvert and extrovert agents. But the machine-like speaking agent is rated lower. \cite{roy2021users}\cmt{[71]}
* usefulness depended on the task \cite{haas2022keep}\cmt{[78]}
* High extroversion chatbot personality have the higher user engagement \cite{moilanen2022measuring}\cmt{[82]}

\textbf{Disfluency}
* Using moderately repetitive utterances resulted in higher desire to continue the dialogue with the agent. \cite{yang2021effect}\cmt{[72]}
* Interjections did not have any impacts of cognitive workload \cite{ceha2022expressive}\cmt{[77]}

\textbf{Proactive}
* Different framing has impacts on the perceived disruptiveness of the agent. \cite{xiao2021let}\cmt{[73]}

\textbf{Prosody}
* No impact on annoying, undesirable, useful \cite{jestin2022effects}\cmt{[81]}
* Dialog-style conversation using prosodic cues is rated as less easy to understand than the reading-style \cite{misu2011toward}\cmt{[83]}

\textbf{Combined}
* (Extraverted: emoticons, self-disclosure, talkative, affective language / Introverted: formal style, less talkative) all personalities have similar usability ratings. Also, users have greater desire to interact with the extraverted chatbot \cite{volkel2022user}\cmt{[75]}
* (Politeness: affective language + elaborateness of speech) no impact on usability \cite{hu2022polite}\cmt{[76]}


%%
\subsubsection{Accuracy}
%%
There are only a few studies that measured the perception of accuracy of conversational agents for an interaction. A couple of studies reported the effect of the use of language style of \textbf{lexical alignment}. One study of a text-based CA resulted in higher perceived response accuracy \cite{huiyang2022improving}\cmt{[17]}, while the other study with voice-based CA did not find any significant differences \cite{linnemann2018can}\cmt{[15]}. Dubiel et al.'s study \cite{dubiel2020persuasive}\cmt{[60]} on \textbf{prosody} did not find any significant differences on the rating for accuracy across agents with different speech rates and pitch.

\textbf{Social talk}
* Introverted agent is more efficient. \cite{roy2021users}\cmt{[71]}
* Efficiency depended on the task \cite{haas2022keep}\cmt{[78]}

\textbf{Language Formality}
* commanding/formal wording was rated significantly more effective than informal wording \cite{jestin2022effects}\cmt{[81]}

\textbf{Prosody}
* No impact on effectiveness \cite{jestin2022effects}\cmt{[81]}

\textbf{Repair}
* One reason for choosing repair strategy preference is efficiency and efficacy. \cite{ashktorab2019resilient}\cmt{[88]}

%%
\subsubsection{Satisfaction}
%%
Several research have investigated into how the usage of \textbf{affective language} impacts the satisfaction people feel when interacting with a CA. Compared to a text-based chatbot with a static and neutral response, users reported higher service encounter satisfaction conversing with a chatbot that responded with dynamic, sentiment-adaptive responses \cite{diederich2019emulating}\cmt{[25]}. However, the same effect is not observed for a voice-based CA as Yang et al. did not find any difference in user satisfaction rating between the emotional-expressing agent vs. non-emotional expressing agent \cite{yang2017perceived}\cmt{[44]}. On the language style used by CAs, both \textbf{disfluency} \cite{pfeifer2009should}\cmt{[12]} and \textbf{emoticons} \cite{wilhelm2022keep}\cmt{[28]} did not have any significant effects on user satisfaction.

Studies have found that presentation formats have significant effects on user satisfaction. In Choi et al's study \cite{choi2020nobody}\cmt{[54]} on the speech rate aspect o f\textbf{prosody} for a voice-based agent, they found that user satisfaction is higher for the default rate compared to using a faster rate. User feedback reflected that the faster rate was mechanical and harder to understand. For text-based chatbots, the addition of \textbf{response delay} increased the perception of overall user satisfaction, as the timing is similar to human-human communications and felt right \cite{gnewuch2018faster}\cmt{[19]}. 

\textbf{Style: Language Formality}
* Modern language had higher usability score compared to Shakespearean language \cite{elsholz2019exploring}\cmt{[61]}
* There is no significant effect on customer satisfaction of the CA between low-status and high-status languages. \cite{habler2019effects}\cmt{[63]}

\textbf{Format: Prosody}
* There is no significant effect on customer satisfaction of the CA between male-gendered and female-gendered voices. \cite{habler2019effects}\cmt{[63]}

\textbf{Social talk}
* Not significant importance between introvert and extrovert agents. But the machine-like speaking agent is rated lower. \cite{roy2021users}\cmt{[71]}

\textbf{Combined}
* (Politeness: affective language + elaborateness of speech) no impact on user satisfaction \cite{hu2022polite}\cmt{[76]}

%%%%%
\subsection{Perception of Agent's Ability}
%%%%%

This section discusses how user perceives the agent's ability in conditions with different conversational architecture elements. It is aiming at measuring the differences in the perception with similar system's capabilities. Out of the two variables used to evaluate this perception, \textit{Intelligence} measures whether the agent is knowledgeable, sensible, and is seem as overall intelligent. \textit{Competence} on the other hand measures the overall perception on how well the agent is handling the conversation. Out of the list of publications that were evaluated, there are only a few papers that measured the perceived ability of an agent.

\textit{To be incorporated}

\textbf{Repair}
* perceived quality of the interaction is muhc higher in repair condition when the agent made a mistake; rating is lower if no mistake was present. \cite{cuadra2021my}\cmt{[67]}

%%
\subsubsection{Intelligence}
%%

On analyzing the impact of different language styles on the perception of intelligence, a couple of papers studied the effect of including \textbf{disfluencies} into agent's speech. One paper found that there is no significant impact on how knowledgeable the agent is perceived as \cite{pfeifer2009should}\cmt{[12]}. Another study showed that using fillers like "um" and "uh" decreased the agent's perceived intelligence overall \cite{jeong2019exploring}\cmt{[10]}. Upon further analysis, the authors noticed a difference between task vs. social conversations, as users perceive the intelligence to be lower in task-oriented situations, but the perception of intelligence is slightly increased in social-oriented situations. The use of \textbf{lexical alignment} did not seem to impact user's perception of an agent's intelligence, as there is no significant difference reported in the agent's domain knowledge between the aligned vs. non-aligned conditions.

On the impact of presentation format, Dubiel et al's study on \textbf{prosody} using different speech rates and pitches did not yield any significant differences on the agent rating for knowledgeable.

\textbf{Prosody}
* VA using kin's voice is rated as more intelligent. \cite{chan2021kinvoices}\cmt{[74]}

\textbf{Repair}
* One reason for choosing repair strategy preference is perceived intelligence and capability. \cite{ashktorab2019resilient}\cmt{[88]}

%%
\subsubsection{Competence}
%%

There are only a couple of papers that discussed the impact of conversational architecture elements on the perception of competence. For \textbf{affective language} in the language style content category, participants rated the agent higher in competence when it has higher patiency and expressed its feelings to users. In Cox et al's study on \textbf{formality}, the authors did not find a significant difference in competence between using casual vs. formal conversational styles.

\textbf{Style: Language Formality}
*  Low-status language resulted in higher achieved performance ratings \cite{habler2019effects}\cmt{[63]}

\textbf{Format: Prosody}
* There is no significant effect on perceived performance of the CA between male-gendered and female-gendered voices. \cite{habler2019effects}\cmt{[63]}
* For expertise, multilingual students rated higher the non-native English-speaking virtual human. \cite{feijoo2021effects}\cmt{[70]}

\textbf{Format: Proactivity}
* Certain proactive styles like notification strategy (conservatively proactive, letting participants know that the agent has found a solution) was evaluated as significantly higher perceived competence compared to the non-strategy. \cite{kraus2020effects}\cmt{[64]}



%%%%%
\subsection{Perception of Social Connection with an Agent}
%%%%%

This section discusses user's perception of their social connection with a conversational agent. Social presence, conversation tone, and intimacy are the three variable used to evaluate this perception. \textit{Social presence} measures user's perceived social distance with an agent, such as social presence, connectedness, and psychological distance. \textit{Conversation tone} measures user perceptions on the way an agent is speaking to them, such as appropriateness of tone, empathetic, or emotionally expressive. \textit{Intimacy} measures the quality of relationship users have with CAs, such as intimacy, social attraction, or similarity with each other.

Out of the list of publications that were evaluated, there is overall consensus that the use of affective language positive affects the perception of social connection with an agent. Conversational architecture elements related to the format of the conversation, like adding response delays, or changing the pitch of speech are commonly investigated for perception of social connection with an agent. There are only a few papers exploring the effect of language styles on the perceived social connection with an agent. 

%%
\subsubsection{Social Presence}
%%
For conversational architecture elements related to language content, studies have shown the use of \textbf{affective language} increased the perception of social presence of an agent. In Diederich et al's study, participants conversing with a text-based agent using dynamic, sentiment-adaptive responses reported a higher social presence compared to a CA that provides static, neutral responses \cite{diederich2019emulating}\cmt{[25]}. In another study, participants felt closer to the agent psychologically when the CA expressed its feelings \cite{lee2019s}\cmt{[55]}. On the use of \textbf{humour}, Lee et al. did not find any significant difference in social presence between the humourous and non-humourous agent \cite{lee2019s}\cmt{[55]}.

For text-based conversational agents, including a \textbf{response delay} increased users' perception of social presence of the agent, with participants feeling a sense of human contact and sociability with the agent \cite{kim2020can}\cmt{[24]}. Westerman et al. \cite{westerman2019believe}\cmt{[9]} studied the presence of typos and capitalized words in a text-based agent's responses. They found the inclusion of \textbf{typos} negatively impacted the social distance and connection with an agent, while \textbf{capitalization} did not have any significant effects. On the effect \textbf{prosody} on a voice-based agent, using an affective tone of voice resulted in perceived higher social connectedness with the agent \cite{kim2020can}\cmt{[24]}.

\textbf{Content: Affective Language (Voice)}
* Model aiming to elicit position emotions from users received higher emotional connection \cite{lubis2019positive}\cmt{[43]}

\textbf{Format: Response Delay}
* Delayed response is associated with higher social presence \cite{gnewuch2022opposing}\cmt{[20]}

\textbf{Content: Affective language and Style: Disfluency}
* Rated higher that agent listened openly to participants' emotions, as that agent can feel waht the user is feeling. \cite{hu2021enhancing}\cmt{[56]}
* social condition didn't have an impact on social presence \cite{lubold2016effects}\cmt{[86]}

\textbf{Prosody}
* VA using kin's voice is rated as more socially present and closer psychological connection with user. \cite{chan2021kinvoices}\cmt{[74]}
* voice plus social condition has higher social presence than the social condition \cite{lubold2016effects}\cmt{[86]}

\textbf{Humour}
* Familiarity with the agent is rated higher at the end of the study (day 7) compared to non-humorous agent \cite{go2021conversational}\cmt{[80]}

\textbf{Laughter}
* Laughter had no impact on social presence of an agent \cite{niewiadomski2013laugh}\cmt{[85]}

%%
\subsubsection{Conversation Tone}
%%

The use of \textbf{affective language} has a significant affect on the perception of the conversation tone by the agent. Multiple studies have found that agents conversing with more affective words are seen as more emotionally expressive and empathetic \cite{daher2020empathic}\cmt{[58]}\cite{diederich2019emulating}\cmt{[25]}\cite{yang2017perceived}\cmt{[44]}\cite{zhu2022effects}\cmt{[26]}. This effect has been observed across modalities in both text-based as well as voice-based conversational agents.

On the style of language used by the agent, \textbf{formality} did not seem to impact the conversation tone of agent as Cox et al. \cite{cox2022does}\cmt{[27]} did not find any significant difference between casual and formal conversational styles for appropriateness of tone. The speech format of \textbf{prosody} seem to have an impact on the conversation tone, as a CA speaking with a lower pitch and slower speech rate is rated as more persuasive compared to other voice settings \cite{dubiel2020persuasive}\cmt{[60]}. However, there was no significant difference in how powerful or bold the agent's tone was perceived across various prosody settings.

\textbf{Content: Affective language and Style: Disfluency}
Respond with empathy to the user. \cite{hu2021enhancing}\cmt{[56]}

\textbf{Disfluency}
* Using moderately repetitive utterances resulted in higher perceived empathy. \cite{yang2021effect}\cmt{[72]}

\textbf{Language Formality}
* commanding/formal wording was rated significantly more appropriate and assertive than informal wording \cite{jestin2022effects}\cmt{[81]}

\textbf{Prosody}
* No impact on appropriate and assertive \cite{jestin2022effects}\cmt{[81]}
* Dialog-style conversation using prosodic cues is rated as more appropriate than the reading-style \cite{misu2011toward}\cmt{[83]}

%%
\subsubsection{Intimacy}
%%
On the use of language content, a conversational agent that included \textbf{self-disclosure} when conversing with participants resulted in higher level of intimacy over time than CAs that did not self disclose \cite{lee2020hear}\cmt{[23]}. On the other hand, using the style of \textbf{lexical alignment} by using the words that were already used by the participant did not result in higher quality of relationship with the agent \cite{linnemann2018can}\cmt{[15]}.

There are several studies investigating the effect of conversational format on intimacy with a CA. Specifically on voice \textbf{prosody}, using an affective tone of voice increased the perceived intimacy and similarity with an agent \cite{kim2020can}\cmt{[24]}. Comparing voice agents with different \textbf{speech rates}, users rated the default rate as more intimate than a faster rate of speech \cite{choi2020nobody}\cmt{[54]}, noting that the default rate is more human-like and natural. Westerman et al's study on different text formats of conversation \cite{westerman2019believe}\cmt{[9]} showed that \textbf{typos} negatively impacted the perceived social attraction of the agent, while \textbf{capitalization} did not have any impacts.

\textbf{Disfluency}
* Interjections did not have any impacts of quality of relationship, but it did result in higher emotional rapport. \cite{ceha2022expressive}\cmt{[77]}

\textbf{Affective Language}
* social condition didn't have an impact on social presence \cite{lubold2016effects}\cmt{[86]}

\textbf{Prosody}
* voice and social condition didn't have an impact on social presence \cite{lubold2016effects}\cmt{[86]}

%%%%%
\subsection{Perception of Agent's Humanness}
%%%%%

This section discusses user's perception of the agent's humanness aspects. Human-likeness, personality traits, and trustworthiness are the three variable used to evaluate this perception. \textit{Human-likeness} measures the agent's similarity to humans as well assessed naturalness of interaction. \textit{Personality traits} measures perceptions of the agent's characteristics such as likeability, warmth, and friendliness. \textit{Trustworthiness} measures if users feel they can trust an agent, and whether the agent is perceived of being truthful.

Out of the list of publications that were evaluated, many papers exploring the impact of language content as well as language style have measurements related to the perception of an agent's humanness. There seems to be fewer studies looking into the effect of conversation format on the perception of agent's humanness, as our reivew only found two studies related to voice prosody. Out of reviewed conversational architecture elements, one consistent finding is that the style of being lexically aligned with user's utterance did not have any impact on the perception of agent's humanness.

%%
\subsubsection{Human-likeness}
%%

On the use of \textbf{affective language}, one study found that CAs with dynamic, sentiment-adaptive responses is perceived as more human than a CA that provides static, neutral responses \cite{diederich2019emulating}\cmt{[25]}. Another study using more emotional expressive words in conversation did not find any significant impact on the perceived human-likeness of an agent \cite{zhu2022effects}\cmt{[26]}.

Different language styles have different impacts on user perceptions. On language \textbf{formality}, agents using normal style was rated as more human-like compared to formal styles of conversation \cite{ouchi2019should}\cmt{[59]}. \textbf{Disfluencies} did not seem to have a significant impact on the perceived humanness or naturalness of an agent \cite{jeong2019exploring}\cmt{[10]}\cite{pfeifer2009should}\cmt{[12]}. However, one study noticed a non-significant but positive trend in user's evaluation of the agent using fillers as being more human-like \cite{jeong2019exploring}\cmt{[10]}.

On the impact of different conversation formats, Zhu et al. did not find a significant effect of \textbf{prosody} on the perception of humanness between agent using expressive vs. non-expressive voices. For text-based CAs, the use of \textbf{typos} made the agent seem less human, while the use of \textit{capitalization} did not have any impacts on the perception of humanness \cite{westerman2019believe}\cmt{[9]}. Lastly, adding a \textbf{response delay} to a CA has a significant positive effect in the perceived humanness of the agent \cite{gnewuch2018faster}\cmt{[19]}. 


\textbf{Style: Disfluency}
* (Voice) No significant difference in naturalness rating for About Myself text, but showed that naturalness is higher for fluent system compared to disfluent speech \cite{wester2015artificial}\cmt{[14]}.

\textbf{Prosody}
* VA using kin's voice is rated as more human-like. \cite{chan2021kinvoices}\cmt{[74]}
* Dialog-style conversation using prosodic cues is rated as less human-like than the reading-style \cite{misu2011toward}\cmt{[83]}

\textbf{Social talk / Elaborateness}
* Naturalness depends on the task \cite{haas2022keep}\cmt{[78]}

\textbf{Language Formality}
* No impact on artificial or humanlike perceptions \cite{jestin2022effects}\cmt{[81]}

\textbf{Prosody}
* Matthew voice is rated as least artificial  \cite{jestin2022effects}\cmt{[81]}

\textbf{Laughter}
* Laughter was not rated as natural in an agent \cite{niewiadomski2013laugh}\cmt{[85]}

\textbf{Combined}
* (Politeness: affective language + elaborateness of speech) no impact on likability, but positive politeness interactions are rated as more polite \cite{hu2022polite}\cmt{[76]}

\textbf{Repair}
* One reason for choosing repair strategy preference is naturalness. \cite{ashktorab2019resilient}\cmt{[88]}


%%
\subsubsection{Personality Traits}
%%

There are significant difference found in the effect of language content on the perception of agent's personality traits. Conversational agents using emotionally expressive words are rated as more likable \cite{zhu2022effects}\cmt{[26]} and warm \cite{lee2019s}\cmt{[55]}. In Healey et al's study \cite{healey2013relating}\cmt{[39]}, there is a significant difference in the variances of the responses for friendliness of the agent between the agent using encouraging words compared to neutral utterances. As with the use of \textbf{humour}, participants rated the humorous agent as having a sense of humor, but it was not rated as more likable \cite{ceha2021can}\cmt{[57]}.

On the impact of language style, various studies on \textbf{lexical alignment} showed that there is no significant impact on the perception of the personality traits related to likability \cite{huiyang2022improving}\cmt{[17]}\cite{linnemann2018can}\cmt{[15]}, friendliness \cite{spillner2021talk}\cmt{[18]}, or politeness \cite{spillner2021talk}\cmt{[18]}. Studies on the \textbf{formality} of language has demonstrated that casual conversational style is perceived as warmer than formal style \cite{cox2022does}\cmt{[27]}. Also agents using a normal style as compared to a formal style is perceived as more friendly, fun, and kind \cite{ouchi2019should}\cmt{[59]}. As for the use of \textbf{disfluency}, there was no statistical significance found in the perceived likability rating \cite{jeong2019exploring}\cmt{[10]}\cite{pfeifer2009should}\cmt{[12]}. Jeong et al. \cite{jeong2019exploring} found a significant interaction effect between disfluency (using fillers) and the context of the conversation (task vs. social), as users considered agents using fillers as entertaining and fun in social conversations, but inappropriate in task oriented situations.

In the list of reviewed papers, there was only one study measuring the perceived trustworthiness of an agent based on different conversational format of \textbf{prosody}. Dubiel et al. found that an agent using slower speech rate is perceived as more trustworthy as compared to other voice prosody settings \cite{dubiel2020persuasive}\cmt{[60]}.

\textit{To be incorporated}

\textbf{Content: Humour}
* An agent that uses humourous content is assessed to be more funny \cite{khooshabeh2011does}\cmt{[37]}.

\textbf{Style: Disfluency}
* (Voice) Both About Myself and Speed Dating ratings has shown system scored significantly differently on the Big-5 personality traits. Disfluency makes the voice sound more neurotic, less open, less extrovert and less conscientious \cite{wester2015artificial}\cmt{[14]}.

\textbf{Format: Prosody}
* Low voice pitch rated not as friendly as the other ones. No impact on other traits like assertive and politeness. \cite{tolmeijer2021female}\cmt{[62]}
* VA using kin's voice is rated as more likable. \cite{chan2021kinvoices}\cmt{[74]}
* No impact on entertaining or friendly  \cite{jestin2022effects}\cmt{[81]}

\textbf{Repair}
* When VA corrected in the condition it didn't make a mistake, it is seen as anxious. Otherwise it is perceived as calm and emotionally stable. \cite{cuadra2021my}\cmt{[67]}
* One reason for choosing repair strategy preference is politeness. \cite{ashktorab2019resilient}\cmt{[88]}

\textbf{Social talk}
* Using more social talk is positive correlated with extroversion. \cite{volkel2021manipulating}\cmt{[68]}
* Not significant importance between introvert and extrovert agents. \cite{roy2021users}\cmt{[71]}
* Likability depends on the task \cite{haas2022keep}\cmt{[78]}

\textbf{Language Formality}
* Informal wording was rated as strongly more entertaining and friendly than the commanding/formal wording. \cite{jestin2022effects}\cmt{[81]}

\textbf{Agreeableness}
* Being highly confrontational (less agreeable) is rated as less conscientious \cite{volkel2021manipulating}\cmt{[68]}
* Designing for more agreeable resulted in personality rated as more agreeable \cite{volkel2021examining}\cmt{[69]}

\textbf{Combined}
* (Extraverted: emoticons, self-disclosure, talkative, affective language / Introverted: formal style, less talkative) extraverted personality rated as more extraverted \cite{volkel2022user}\cmt{[75]}

%%
\subsubsection{Trustworthiness}
%%

For the use of language content, agents using more \textbf{affective language} is seen as less trustworthy, as users felt it was being overly familiar \cite{andrews2012system}\cmt{[38]}. On the other hand, users conversing with agents that used \textbf{self-disclosure} content reported more trust towards the agent compared to the non disclosure group \cite{lee2020hear}\cmt{[23]}.

None of the studies included in this review has found any significant impact of language style, \textbf{lexical alignment} \cite{hoegen2019end}\cmt{[31]}\cite{huiyang2022improving}\cmt{[17]}\cite{linnemann2018can}\cmt{[15]}, \textbf{disfluency} \cite{pfeifer2009should}\cmt{[12]}, and \textbf{emoticons} \cite{wilhelm2022keep}\cmt{[28]} on the perceived trustworthiness of CAs. Similarly for conversational format, there is not significant difference on the rating for truthfulness of the agent across different voice \textbf{prosody} settings \cite{dubiel2020persuasive}\cmt{[60]}.

\textbf{Format: Prosody}
* No significant difference in trust ratings across different voice prosody settings with differences in pitch. \cite{tolmeijer2021female}\cmt{[62]}
* No impact on trustworthiness  \cite{jestin2022effects}\cmt{[81]}

\textbf{Format: Proactivity}
* Overall trust in the system did not differ significantly. The intervention style is less trusted than conservative strategies - too obstructive and imposing. Proactive should be subtle  \cite{kraus2020effects}\cmt{[64]}

\textbf{Language Formality}
* No impact on trustworthiness \cite{jestin2022effects}\cmt{[81]}

